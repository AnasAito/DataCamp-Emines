{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from typing import Tuple, List\n",
    "import time\n",
    "from os import path\n",
    "import requests\n",
    "\n",
    "import urllib\n",
    "import zipfile\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgFyk78PKY_J"
   },
   "source": [
    "# Exercice 1 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2a7fd_1KY_O"
   },
   "outputs": [],
   "source": [
    "url = u'https://archive.ics.uci.edu/ml/machine-learning-databases/00235/'\n",
    "filename = 'household_power_consumption'\n",
    "zipfilename = filename + '.zip'\n",
    "location = url + zipfilename\n",
    "if not path.isfile(zipfilename):\n",
    "    urllib.request.urlretrieve(location, zipfilename)\n",
    "zipfile.ZipFile(zipfilename).extractall()\n",
    "na_values = ['?', '']\n",
    "fields = ['Date', 'Time', 'Global_active_power']\n",
    "df = pd.read_csv(filename + '.txt', sep=';', nrows=200000,\n",
    "                 na_values=na_values, usecols=fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLfFbNknKY_Q"
   },
   "source": [
    "## Question 1 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3ABOR98KY_R",
    "outputId": "6b38a460-7645-44a8-a2df-3ffbe82c1a37"
   },
   "outputs": [],
   "source": [
    "# count missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqvSOD51KY_S"
   },
   "outputs": [],
   "source": [
    "# delete rows with missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmG1XMoMKY_T",
    "outputId": "928f5435-c851-46a5-e89b-82addb6a640f"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1nu0A8_KY_U"
   },
   "source": [
    "# Question 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPS87o-YKY_U"
   },
   "outputs": [],
   "source": [
    "df['DateTime'] = df['Date'] +' '+ df['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtqS0n9rKY_V"
   },
   "outputs": [],
   "source": [
    "# datetime to timetime \n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyeFADLKKY_W"
   },
   "outputs": [],
   "source": [
    "# set index to DateTime\n",
    "df = df.set_index('DateTime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbZCGnSUKY_W"
   },
   "outputs": [],
   "source": [
    "# Time and Date are now useless \n",
    "df.drop(['Date', 'Time'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "th9Crd9zKY_W",
    "outputId": "33f4fd84-8657-406c-b07a-80de01f41da0"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbgAWYGZKY_X"
   },
   "source": [
    "# question 3 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbIZaJkeKY_X",
    "outputId": "41bafce3-4a2c-4e57-860d-8db789e6469c"
   },
   "outputs": [],
   "source": [
    "# average power consumption per day \n",
    "df_daily = df.resample('D').mean()\n",
    "\n",
    "# plot global_active_power between january 1 2007 and april 30 2007 \n",
    "df_daily.loc['2007-01-01':'2007-04-30'].plot(y='Global_active_power')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzSjgRc6KY_Y"
   },
   "source": [
    "## Question 4 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3KSTbDuKY_Y",
    "outputId": "d6ed4888-155a-46df-8316-a859455b4ddc"
   },
   "outputs": [],
   "source": [
    "\n",
    "# load https://perso.telecom-paristech.fr/sabourin/mdi720/TG_STAID011249.txt\n",
    "url = 'https://perso.telecom-paristech.fr/sabourin/mdi720/TG_STAID011249.txt'\n",
    "response = requests.get(url)\n",
    "temp_txt = response.text\n",
    "\n",
    "# some custom code to clean text file could be done manually ... \n",
    "temp_txt = temp_txt.split('#See files sources.txt and stations.txt for more info.')[-1].split('#')[-1]\n",
    "temp_txt = temp_txt.split('\\n')\n",
    "temp_txt = [line.replace('\\r','').strip() for line in temp_txt][1:-1]\n",
    "temp_header = [col.strip() for col in temp_txt[0].split(',')]\n",
    "def clean_line(line):\n",
    "    line = line.split(',')\n",
    "    line = [col.strip() for col in line]\n",
    "    # to numeric \n",
    "    line = [float(col) for col in line]\n",
    "    # date\n",
    "    line[2] = str(int(line[2]))\n",
    "    return line\n",
    "temp_list = [clean_line(row) for row in temp_txt[1:]]\n",
    "\n",
    "df_temp = pd.DataFrame(temp_list, columns=temp_header)\n",
    "df_temp = df_temp[['DATE', 'TG']]\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tj3GzrqGKY_Y"
   },
   "outputs": [],
   "source": [
    "# temp to celcius \n",
    "df_temp['TG'] = df_temp['TG']/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emuxRSweKY_Y",
    "outputId": "c30b6741-b8b0-4a7b-bfce-a4e256363e85"
   },
   "outputs": [],
   "source": [
    "# check missing values\n",
    "df_temp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EjRj3hKKY_Z"
   },
   "outputs": [],
   "source": [
    "# timestamp to datetime\n",
    "df_temp['DATE'] = pd.to_datetime(df_temp['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sO0W_fZKY_Z"
   },
   "outputs": [],
   "source": [
    "# reset index\n",
    "df_temp = df_temp.set_index('DATE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_aTKSmXKY_Z",
    "outputId": "3ef9d8d5-b3ca-4c6b-e3ce-f2917a95c8e5"
   },
   "outputs": [],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E5sdGq4KY_Z"
   },
   "source": [
    "## Question 5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CH0TeQsxKY_Z",
    "outputId": "d945b706-7e07-40f4-bbc8-8436a71dc05d"
   },
   "outputs": [],
   "source": [
    "# merge with df_daily\n",
    "df_daily = df_daily.join(df_temp, how='inner')\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQO_LjomKY_a",
    "outputId": "bffedb0b-c221-4b14-ac59-c89758801156"
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot temperature and Global_active_power  between january 1 2007 and april 30 2007 \n",
    "df_filter = df_daily.loc['2007-01-01':'2007-04-30']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "df_filter['Global_active_power'].plot(ax=ax, style='b-',label='Global_active_power')\n",
    "df_filter['TG'].plot(ax=ax, style='r-', secondary_y=True, label ='temperature')\n",
    "\n",
    "ax.legend([ax.get_lines()[0], ax.right_ax.get_lines()[0]], ['Global_active_power','TG'], loc=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9snHzjMKY_a"
   },
   "source": [
    "- we can see a negative correlation between the temperature  and consuption \n",
    "which is expected since we use electricity for heating (a high energy demanding appliance) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_left_right_sides(n: int, p: int, lbd: float) -> Tuple[np.ndarray, float, np.ndarray, float]:\n",
    "    # matrices\n",
    "    X = np.random.normal(\n",
    "        loc=0, \n",
    "        scale=5**2,\n",
    "        size=(n, p)\n",
    "    )\n",
    "\n",
    "    y = np.random.uniform(\n",
    "        low=-1, \n",
    "        high=1, \n",
    "        size=(n, 1)\n",
    "    )\n",
    "\n",
    "    id_n = np.eye(n)\n",
    "    id_p = np.eye(p)\n",
    "\n",
    "    # operations and inv\n",
    "    # left\n",
    "    start_time = time.time()\n",
    "\n",
    "    transpose_with_id_n = X @ X.T + lbd * id_n\n",
    "    transpose_id_n_times_y = np.linalg.solve(transpose_with_id_n, y)\n",
    "    left_side = X.T @ transpose_id_n_times_y\n",
    "\n",
    "    left_duration = time.time() - start_time\n",
    "\n",
    "    # right\n",
    "    start_time = time.time()\n",
    "\n",
    "    transpose_with_id_p = X.T @ X + lbd * id_p\n",
    "    transpose_id_p_times_xy = np.linalg.solve(transpose_with_id_p, X.T @ y)  \n",
    "    right_side = transpose_id_p_times_xy\n",
    "\n",
    "    right_duration = time.time() - start_time\n",
    "\n",
    "    return left_side, left_duration, right_side, right_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. a)\n",
    "\n",
    "# params\n",
    "n, p = 100, 2000\n",
    "lbd = 1e-5\n",
    "\n",
    "l_side, _, r_side, _ = get_left_right_sides(n, p, lbd)\n",
    "\n",
    "# check equality\n",
    "all(np.isclose(l_side, r_side, atol=1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. b)\n",
    "\n",
    "# params\n",
    "n, p = 2000, 100\n",
    "lbd = 1e-5\n",
    "\n",
    "l_side, _, r_side, _ = get_left_right_sides(n, p, lbd)\n",
    "\n",
    "# check equality\n",
    "all(np.isclose(l_side, r_side, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for this case, the equality holds for an absolute tolerence greater than $10^{-6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_duration(n, p):\n",
    "    _, duration_left, _, duration_right = get_left_right_sides(n, p, lbd)\n",
    "\n",
    "    return (duration_left, duration_right)\n",
    "\n",
    "\n",
    "# samples of cases\n",
    "case_a = [(i, i * 20) for i in range(100, 200+1, 10)]\n",
    "case_b = [(i * 20, i) for i in range(100, 200+1, 10)]\n",
    "\n",
    "  \n",
    "times_case_a = list(\n",
    "    map(\n",
    "        lambda tup: get_duration(*tup), \n",
    "        case_a\n",
    "    )\n",
    ")\n",
    "\n",
    "times_case_b = list(\n",
    "    map(\n",
    "        lambda tup: get_duration(*tup), \n",
    "        case_b\n",
    "    )      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "# init figure\n",
    "fig = make_subplots(\n",
    "    rows=1, \n",
    "    cols=2,\n",
    "    subplot_titles=[\"duration for case n << p\", \"duration for case n >> p\"]\n",
    ")\n",
    "\n",
    "color_name = {\n",
    "    0: {\n",
    "        \"name\": \"left method\",\n",
    "        \"color\": '#636EFA'\n",
    "    },\n",
    "    1: {\n",
    "        \"name\": \"right method\",\n",
    "        \"color\": '#EF553B'\n",
    "    }\n",
    "}\n",
    "\n",
    "fig.add_traces(data=[\n",
    "    go.Bar(\n",
    "        x=[str(el) for el in case_a],\n",
    "        y=[tuple_duration[comp_method] for tuple_duration in times_case_a],\n",
    "        marker_color=color_name[comp_method][\"color\"],\n",
    "        name=color_name[comp_method][\"name\"],\n",
    "    )\n",
    "for comp_method in [0, 1]], \n",
    "rows=1, cols=1\n",
    ")\n",
    "\n",
    "fig.add_traces(data=[\n",
    "    go.Bar(\n",
    "        x=[str(el) for el in case_b],\n",
    "        y=[tuple_duration[comp_method] for tuple_duration in times_case_b],\n",
    "        marker_color=color_name[comp_method][\"color\"],\n",
    "        name=color_name[comp_method][\"name\"],\n",
    "        showlegend=False\n",
    "    )\n",
    "for comp_method in [0, 1]], \n",
    "rows=1, cols=2\n",
    ")\n",
    "\n",
    "# set layout\n",
    "for i in [1, 2]:\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"time\", \n",
    "        type=\"log\", \n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is time efficient to use the left method in the case $n << p$. On the other hand, it turns out that\n",
    "the right method is the one to opt for in case $n >> p$\n",
    "\n",
    "- In addition, if we analyse the time complexity of the two operation (without taking on account inv operation)\n",
    "    - left method: $O(n^2 \\ p)$\n",
    "    - right method: $O(p^2 \\ n)$\n",
    "\n",
    "which shows clearly cases where to choose the method to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosen distributions\n",
    "DICT_DIST = {\n",
    "        \"uniform\": {\n",
    "            \"func\": np.random.uniform,\n",
    "            \"mean\": 1/2,\n",
    "            \"sigma\": np.sqrt(1/12),\n",
    "            \"color\": '#636EFA'\n",
    "        },\n",
    "        \"exponential\": {\n",
    "            \"func\": np.random.exponential,\n",
    "            \"mean\": 1.,\n",
    "            \"sigma\": 1.,\n",
    "            \"color\": '#EF553B'\n",
    "        },\n",
    "        \"poisson\": {\n",
    "            \"func\": np.random.poisson,\n",
    "            \"mean\": 1.,\n",
    "            \"sigma\": 1.,\n",
    "            \"color\": '#00CC96'\n",
    "        },\n",
    "    }\n",
    "\n",
    "TARGET_MEAN = 0\n",
    "TARGET_VARIANCE = 2\n",
    "\n",
    "\n",
    "def sample_from(n: int, p: int, dist_name: str) -> np.ndarray:\n",
    "    # select dist\n",
    "    dist = DICT_DIST[dist_name]\n",
    "    # sample from it\n",
    "    samples_dist = dist[\"func\"](size=(n, p))\n",
    "\n",
    "    # map it to (mean = 0, var=2)\n",
    "    return np.sqrt(TARGET_VARIANCE) * ((samples_dist - dist[\"mean\"]) / dist[\"sigma\"] + TARGET_MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical check of mean and variance\n",
    "interval_n = [10 ** i for i in range(1, 6+1)]\n",
    "\n",
    "diffirence_to_mean = {\n",
    "    dist_name: [abs(sample_from(n, 1, dist_name).mean() - TARGET_MEAN) for n in interval_n]\n",
    "    for dist_name in DICT_DIST\n",
    "}\n",
    "\n",
    "diffirence_to_variance = {\n",
    "    dist_name: [abs(sample_from(n, 1, dist_name).var() - TARGET_VARIANCE) for n in interval_n]\n",
    "    for dist_name in DICT_DIST\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plot to compare emperical and theorical mean/variance\n",
    "\n",
    "# init figure\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[f\"abs difference to target mean={TARGET_MEAN}\", f\"abs difference to target variance={TARGET_VARIANCE}\"]\n",
    ")\n",
    "\n",
    "# plots to compare mean\n",
    "fig.add_traces(data=[\n",
    "    go.Scatter(\n",
    "        x=interval_n,\n",
    "        y=diffirence_to_mean[dist_name],\n",
    "        mode=\"markers+lines\",\n",
    "        name=dist_name,\n",
    "        marker_color=DICT_DIST[dist_name][\"color\"]\n",
    "    )\n",
    "for dist_name in DICT_DIST],\n",
    "rows=1, cols=1)\n",
    "\n",
    "# plots to compare variance\n",
    "fig.add_traces(data=[\n",
    "    go.Scatter(\n",
    "        x=interval_n,\n",
    "        y=diffirence_to_variance[dist_name],\n",
    "        mode=\"markers+lines\",\n",
    "        name=dist_name,\n",
    "        marker_color=DICT_DIST[dist_name][\"color\"],\n",
    "        showlegend=False\n",
    "    )\n",
    "for dist_name in DICT_DIST],\n",
    "rows=1, cols=2)\n",
    "\n",
    "# set axis layout\n",
    "for i in [1, 2]:\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"n (samples)\", \n",
    "        type=\"log\", \n",
    "        row=1, col=i\n",
    "    )\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We notice that the more we increase the number of samples, the more we get close to the theorical mean and variance\n",
    "- This remark is due to the law of large numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init figure\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=list(DICT_DIST.keys())\n",
    ")\n",
    "\n",
    "# params\n",
    "n = 1000\n",
    "values_p = [200, 500, 1000, 2000]\n",
    "arr_colors = ['#636EFA', '#EF553B', '#00CC96', '#AB63FA']\n",
    "\n",
    "for i, dist_name in enumerate(DICT_DIST):\n",
    "    for p, color_name in zip(values_p, arr_colors):\n",
    "        # build and apply svd\n",
    "        X = sample_from(n, p, dist_name)\n",
    "        _, singular_vals, _ = np.linalg.svd(X)\n",
    "\n",
    "        # plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                y=singular_vals,\n",
    "                mode=\"markers+lines\",\n",
    "                name=f\"p={p}\",\n",
    "                marker_color=color_name,\n",
    "                showlegend=False if dist_name != \"uniform\" else True\n",
    "            ),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "\n",
    "# set axis layout\n",
    "for i in [1, 2, 3]:\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"singular values\",\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We notice that number of singular values is equal to $min(n, p)$. Indeed, the number of singular values is at most\n",
    "equal to the matrix rank\n",
    "- We observe that singular values have the same shape for all distributions (when we zoom in, we see that they are slightly different)\n",
    "- The latter remark can be justified by the fact that all distribution have $mean=0$ and $variance=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# init figure\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=list(DICT_DIST.keys())\n",
    ")\n",
    "\n",
    "# params\n",
    "n = 1000\n",
    "values_p = [200, 500, 1000, 2000]\n",
    "arr_colors = ['#636EFA', '#EF553B', '#00CC96', '#AB63FA']\n",
    "\n",
    "for i, dist_name in enumerate(DICT_DIST):\n",
    "    for p, color_name in zip(values_p, arr_colors):\n",
    "        # build and apply svd\n",
    "        X = sample_from(n, p, dist_name)\n",
    "        eigen_values, _ = np.linalg.eig(X.T @ X / n)\n",
    "\n",
    "        # plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                y=np.real(eigen_values),\n",
    "                mode=\"markers+lines\",\n",
    "                name=f\"p={p}\",\n",
    "                marker_color=color_name,\n",
    "                showlegend=False if dist_name != \"uniform\" else True\n",
    "            ),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "\n",
    "# set axis layout\n",
    "for i in [1, 2, 3]:\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"eigen values\",\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Like the previous question, we observe that at the moment $p$ becomes greater than $n=1000$, all eigen values\n",
    "becomes zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial implementation\n",
    "def power_method(X: np.ndarray, max_iterations: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # X shape\n",
    "    _, p = X.shape\n",
    "\n",
    "    # random init of v\n",
    "    v_k = np.random.rand(p, 1)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # update u\n",
    "        X_times_v = X @ v_k\n",
    "        u_k = X_times_v / np.linalg.norm(X_times_v)\n",
    "\n",
    "        # update v\n",
    "        XT_times_u = X.T @ u_k\n",
    "        v_k = XT_times_u / np.linalg.norm(XT_times_u)\n",
    "\n",
    "    return u_k, v_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo modified\n",
    "def power_method_modified(X: np.ndarray, max_iterations: int) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "    # X shape\n",
    "    _, p = X.shape\n",
    "\n",
    "    # random init of v\n",
    "    arr_v = [np.random.rand(p, 1)]\n",
    "    arr_u = []\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # update u\n",
    "        X_times_v = (X @ arr_v[-1])\n",
    "        arr_u.append(X_times_v / np.linalg.norm(X_times_v))\n",
    "\n",
    "        # update v\n",
    "        XT_times_u = X.T @ (arr_u[-1])\n",
    "        arr_v.append(XT_times_u / np.linalg.norm(XT_times_u))\n",
    "\n",
    "    return arr_u, arr_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "n, p = 5, 10\n",
    "dist_name = \"uniform\"\n",
    "\n",
    "# create matrix\n",
    "X = sample_from(n, p, dist_name)\n",
    "U, _, V = np.linalg.svd(X)\n",
    "\n",
    "# leading singular vectors\n",
    "u_star = U[:, 0][:, None]\n",
    "v_star = V.T[:, 0][:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of u* - u (resp v* - v)\n",
    "\n",
    "# params\n",
    "nb_iteration = 200\n",
    "\n",
    "arr_u, arr_v = power_method_modified(X, max_iterations=nb_iteration)\n",
    "rg_1000 = list(range(1, nb_iteration + 1))\n",
    "\n",
    "# init figure\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[\"||u* - u||\", \"||v* - v||\"]\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=rg_1000,\n",
    "        y=[np.linalg.norm(u_star - u) for u in arr_u],\n",
    "        mode=\"lines+markers\",\n",
    "        marker_color=\"#636EFA\",\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=rg_1000,\n",
    "        y=[np.linalg.norm(v_star - v) for v in arr_v],\n",
    "        mode=\"lines+markers\",\n",
    "        marker_color=\"#636EFA\",\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# set axis layout\n",
    "for i in [1, 2]:\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"iteration\",\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        type=\"log\",\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see clearly that as we increase the number of iteration, the distance between $u^*$ and $u$ (resp $v^*$ and $v$)\n",
    "converge to $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifed version to approx the max singular value\n",
    "def power_method_singular_value(X: np.ndarray, max_iterations: int) -> float:\n",
    "    # X shape\n",
    "    _, p = X.shape\n",
    "\n",
    "    # random init of v\n",
    "    v_k = np.random.rand(p, 1)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # update u\n",
    "        X_times_v = X @ v_k\n",
    "        u_k = X_times_v / np.linalg.norm(X_times_v)\n",
    "\n",
    "        # update v\n",
    "        XT_times_u = X.T @ u_k\n",
    "        v_k = XT_times_u / np.linalg.norm(XT_times_u)\n",
    "\n",
    "    return np.sqrt(np.linalg.norm((X.T @ X) @ v_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "n, p = 5, 10\n",
    "dist_name = \"uniform\"\n",
    "\n",
    "# create matrix\n",
    "X = sample_from(n, p, dist_name)\n",
    "U, sig, V = np.linalg.svd(X)\n",
    "\n",
    "# singular value using power method\n",
    "sv = power_method_singular_value(X, max_iterations=1000)\n",
    "\n",
    "\n",
    "print(np.isclose(sv, sig[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_power_method(\n",
    "    X: np.ndarray, \n",
    "    ith_singular: int, \n",
    "    max_iterations: int, \n",
    "    v: np.ndarray = None, u: np.ndarray = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "    \n",
    "    # condition to break recussion\n",
    "    if ith_singular == 0:\n",
    "        sig = np.sqrt(np.linalg.norm((X.T @ X) @ v))\n",
    "        return v, u, sig\n",
    "    \n",
    "    # params\n",
    "    if (u is not None and\n",
    "        v is not None):\n",
    "        sig = np.sqrt(np.linalg.norm((X.T @ X) @ v))\n",
    "        X = X - sig * u @ v.T\n",
    "\n",
    "    _, p = X.shape\n",
    "\n",
    "    # random init of v\n",
    "    v_k = np.random.rand(p, 1)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # update u\n",
    "        X_times_v = X @ v_k\n",
    "        u_k = X_times_v / np.linalg.norm(X_times_v)\n",
    "\n",
    "        # update v\n",
    "        XT_times_u = X.T @ u_k\n",
    "        v_k = XT_times_u / np.linalg.norm(XT_times_u)\n",
    "\n",
    "    # get next sv\n",
    "    return gen_power_method(X, ith_singular-1, max_iterations, v_k, u_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "n, p = 5, 10\n",
    "dist_name = \"uniform\"\n",
    "\n",
    "# create matrix\n",
    "X = sample_from(n, p, dist_name)\n",
    "U, sig, V = np.linalg.svd(X)\n",
    "\n",
    "# 2nd sv using power method\n",
    "v, u, sv = gen_power_method(X, ith_singular=2, max_iterations=1000)\n",
    "\n",
    "\n",
    "np.isclose(sv, sig[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The developped function is able to approximate not only the second largest singular value\n",
    "but also the third, fourth, ... ith singular value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qeustion 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Npa8iauEKY_a",
    "outputId": "e308d5d1-fc8c-4b0d-fe73-3919da19dc3a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# load https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original dataframe \n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "header = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin', 'car name']\n",
    "df_auto = pd.read_csv(url, sep='\\s+', na_values='?', header=None, names=header)\n",
    "df_auto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX7Op9nBKY_b",
    "outputId": "222d04a9-860d-450b-e78d-f33da304f7a0"
   },
   "outputs": [],
   "source": [
    "# check missing values\n",
    "df_auto.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvZIN25iKY_b"
   },
   "outputs": [],
   "source": [
    "# drop missing values \n",
    "df_auto.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plwcFsZkKY_b"
   },
   "source": [
    "## Question 17 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUf6h-dwKY_b",
    "outputId": "f1333034-8be5-4572-ffad-32af4da3f594"
   },
   "outputs": [],
   "source": [
    "\n",
    "# add three binary columns : is_US, is_Europe, is_Japan , we will do it in a more ML-ish  way later ...\n",
    "df_auto['is_US'] = df_auto['origin'] == 1\n",
    "df_auto['is_Europe'] = df_auto['origin'] == 2\n",
    "df_auto['is_Japan'] = df_auto['origin'] == 3\n",
    "df_auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPqvAdNaKY_b"
   },
   "source": [
    " ## question 18 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQlyI0l6KY_c",
    "outputId": "b4e081db-b059-4fbd-920a-96135dc1130a"
   },
   "outputs": [],
   "source": [
    "# sample 3 rows from each origin class / we resampled till we found diferent model years ! \n",
    "frame_1 = df_auto[df_auto['is_US'] ==True].sample(3)  \n",
    "frame_2 = df_auto[df_auto['is_Japan'] ==True].sample(3) \n",
    "frame_3 =  df_auto[df_auto['is_Europe'] ==True].sample(3)\n",
    "frames = [frame_1, frame_2, frame_3]\n",
    "\n",
    "result = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-vvXHEEKY_c"
   },
   "outputs": [],
   "source": [
    "# get least square estimator \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train,y_train = result.loc[:, [ 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin', 'is_US','is_Europe','is_Japan']], result['mpg']\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "coefs = regressor.coef_ + regressor.intercept_\n",
    "coefs = list(regressor.coef_)  + [regressor.intercept_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uf0gSjkeKY_c"
   },
   "outputs": [],
   "source": [
    "X_test ,y_test = df_auto.loc[:, [ 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin', 'is_US','is_Europe','is_Japan']],df_auto['mpg']\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rGkCm9wKY_c",
    "outputId": "77de20f0-bdf0-4f49-db66-ac5e0dc81485"
   },
   "outputs": [],
   "source": [
    "# get the rmse \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print('rmse : ', rmse)\n",
    "print('relative rmse to mean : ', round(rmse/np.mean(y_test),2), '%')\n",
    "# quite a high error given the mean of our date we cant acceot a 30 % devieation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUfm9d3sKY_c",
    "outputId": "5175b200-49f4-4980-83e3-1b10dd223464"
   },
   "outputs": [],
   "source": [
    "dict(zip( [ 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'is_US','is_Europe','is_Japan','INTERCEPT'],coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyVon5ZNKY_d"
   },
   "source": [
    "Comments : \n",
    "\n",
    "- american cars are more efficient than european  and japaneese cars...  :)\n",
    "- weight with the least effect on consumption is not realistic ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLsvXCF7KY_d"
   },
   "source": [
    "## question 19 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rnlB7qDKY_d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# for pipeline vis purpose \n",
    "from sklearn import set_config\n",
    "set_config(display='diagram') \n",
    "# sklearn imports \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# define feature used for training \n",
    "one_hot_encoding_columns = ['origin']\n",
    "scaling_columns = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year']\n",
    "# prepare preprocessor \n",
    "# one to onehot encode  the origin column \n",
    "# and another one for scaling num vars \n",
    "preprocessor = ColumnTransformer([\n",
    "\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown='ignore'),\n",
    "     one_hot_encoding_columns),\n",
    "    ('standard-scaler', StandardScaler(), scaling_columns)\n",
    "])\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4qqGan8KY_d",
    "outputId": "414ce490-ccd2-4919-b9db-5c96ad772497"
   },
   "outputs": [],
   "source": [
    "# fit the pipeline \n",
    "x_columns = scaling_columns+one_hot_encoding_columns\n",
    "data_train = df_auto.loc[:,x_columns]\n",
    "target_train = df_auto['mpg']\n",
    "\n",
    "\n",
    "model.fit(data_train, target_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgCsoCoWKY_d",
    "outputId": "0b499c0a-0a06-4cf5-a009-e128f7c48257"
   },
   "outputs": [],
   "source": [
    "# get rmse \n",
    "target_pred = model.predict(data_train)\n",
    "rmse_full = np.sqrt(mean_squared_error(target_train, target_pred))\n",
    "print('rmse : ', rmse_full)\n",
    "print('relative rmse to mean : ', round(rmse_full/np.mean(target_train),2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwbJf6PAKY_d",
    "outputId": "bb9af70b-f7be-45c6-d51b-e6ac7a5557a9"
   },
   "outputs": [],
   "source": [
    "# get lin_reg coeifs \n",
    "coefs_full = list(model.named_steps['linearregression'].coef_) +[ model.named_steps['linearregression'].intercept_]\n",
    "origine_new_feat_names  = list(model.named_steps['columntransformer'] .named_transformers_['one-hot-encoder'].get_feature_names_out())\n",
    "feature_names =   origine_new_feat_names+ scaling_columns+['INTERCEPT']\n",
    "dict(zip(feature_names,coefs_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79Ze0JN1KY_e"
   },
   "source": [
    "Comments :\n",
    "-   given the coefs we can say that the weight is the most important feature wich is realisatic since more weight => more consuption \n",
    "-  we can see also that there is positive dependency between the year and the consumption whch may cause some problems if we traied the model on futuristic years like 2020 or smtg \n",
    "\n",
    "## question 20 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7l7owYCKY_e",
    "outputId": "efccdbab-05f6-4388-bd22-eace925d8317"
   },
   "outputs": [],
   "source": [
    "\n",
    "observation = [6 ,225 ,100 ,3233 ,15.4, 2017-1900, 1]\n",
    "observation = np.array(observation).reshape(1,-1)\n",
    "observation_df = pd.DataFrame(observation, columns=x_columns)\n",
    "prediction = model.predict(observation_df)[0]\n",
    "print('prediction : ', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Ws568bYKY_e",
    "outputId": "a74e2a6d-0aee-4084-a6d0-597099f94ad0"
   },
   "outputs": [],
   "source": [
    "prediction , np.mean(target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibz31eNUKY_e"
   },
   "source": [
    "Comments : (high leverage points)\n",
    "- the high consumption comes from the year which is  out of sample and already have a high positive coef\n",
    " which  drive the prediction up and may be unrealistic . \n",
    "- this is equivalent to outliers (whcih we found in target variable) \n",
    "- this is found in the regressor variableswe call them **high leverage points** "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e915f0a29dc84041eaeb02b7b1a21c440e37a87b61d44d5e84a515737dc82bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
